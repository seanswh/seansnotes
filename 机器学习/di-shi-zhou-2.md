1.保证随机GD的收敛与学习速率的选择

如何确保学习实在朝着有效的风向进行？也即结果是不断收敛的。

    在Batch GD中，我们是画损失函数和GD迭代次数的图像。

    在Stochastic GD中，我们可以在固定的迭代次数后，画成本函数的在这些迭代上的平均值

如下图所示：

![](/机器学习/images/105.PNG)

如果随机迭代每次的采样率过小，则学习曲线容易抖动；如果采样频率过大，则曲线平滑。如果结果不收敛，则说明学习步长太大，如下图所示：![](/机器学习/images/106.PNG)
可以让学习速率随着迭代次数增加而减少，以保证逐渐稳定地收敛到最优解



