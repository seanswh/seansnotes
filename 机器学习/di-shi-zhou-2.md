1.保证随机GD的收敛与学习速率的选择

如何确保学习实在朝着有效的风向进行？也即结果是不断收敛的。

    在Batch GD中，我们是画损失函数和GD迭代次数的图像。

    在Stochastic GD中，我们可以在固定的迭代次数后，画成本函数的在这些迭代上的平均值

如下图所示：

![](/机器学习/images/105.PNG)

如果随机迭代每次的采样率过小，则学习曲线容易抖动；如果采样频率过大，则曲线平滑。如果结果不收敛，则说明学习步长太大，如下图所示：![](/机器学习/images/106.PNG)
可以让学习速率随着迭代次数增加而减少，以保证逐渐稳定地收敛到最优解


2.online learning（在线学习）
在很多机器学习运行的系统中，很多学习数据是不断在产生的，如何可以让系统随着数据的涌入保持学习，而不是只能获得一次性用固定数据训练的模型，以获得更优的性能，就是在线学习所要解决的问题。
![](/机器学习/images/107.PNG)
在线学习系统在每次有新的数据产生（一个新用户访问了网站，并给出是否使用快递服务的决策）时，系统都会对参数进行一次更新。
