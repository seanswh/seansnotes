1.人为扩展数据集  
机器学习模型的训练中，我们往往需要大量的数据。而这些数据从哪里来呢？我们可以用Artificial Data Synthesis（人工数据合成）。  
如下图，假设我们收集了一个文字识别的数据集（图像+字母标记），我们如何将它”扩大“呢？  
![](/机器学习/images/113.PNG)  
对于文字识别来说，可以通过变形、旋转、缩放来增加样本,对于音频来说，可以通过增加噪音来扩大样本。要注意的是，所添加的噪音/扭曲必须是在对应类型的数据集中比较有代表性的噪音/扭曲。

获得更多数据的注意事项：

确认使用的是low bias的分类器（通过画学习曲线来判别）（如果是high bias的分类器，增加样本数量对提升模型性能已经不太有用了【见前面】，这时要增加训练特征数目，比如在神经网络里可以增加隐藏层的神经元数目）  
![](/机器学习/images/114.PNG)  
注意获得更多数据的投入成本。考虑到所付出的工作和模型可能从更多的数据中获得的性能改善，作出权衡。（不同的三种途径：人工合成，自己搜集，众筹）

2.Ceiling Analysis：上限分析

前面学到，机器学习的过程使用到了“流水线"的概念，我们希望在改善机器学习系统的性能时，把更多的精力投入到流水线中性价比较高的地方，即改善的努力最有可能得到回报的部分，那么，我们就需要首先找出当前是系统的哪个部分对系统的性能限制最大。

如下图，回到文字识别问题，我们对识别系统的不同组件（component）的准确度进行对  
比：![](/机器学习/images/115.PNG)  
按照流程的顺序，我们会不断地将每个涉及到模型性能的流程做一些调整，使得当前流程的模型表现“完美”，即通过调整，使某个流程的模型在某个阶段表现100%准确（手工标记正确标签（ground-truth labels），然后将完全正确处理过的数据再输入到下一个模型中）。这时，再测量系统的准确率如何，这个准确率也就是当前流程模型表现“完美”时的系统瓶颈（ceiling）性能。  
如上图所示，我们第一步先通过人工判识方式，挑选出所有样本中的文字所在位置，这样保证流水线中"Text detection" 100%准确，可以看到，系统的准确率从原有72%升到89%，然后再通过人工的方式，分离出所有的字母，这样整体性能从89%上升到90%，以此类推。  
获得了所有流程中的模型的表现完美的情况下，我们就可以开始抉择，在哪些阶段的模型改进上下功夫。当然是在能够使得瓶颈性能获得最大程度的改进的模型上！（上图中，是文字检测模型，因为可以获得最大72%-&gt;89%的大幅度改善。）

