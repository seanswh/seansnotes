当数据量相当大的时候，传统迭代的方式效率太低。假设我们如果有一百万个训练样本（m=100,000,000），则每一次更新参数都需要做一百万次的加和，那么计算量就很大。另外，对于high bias的算法学习曲线（如下图的靠右图），此时增加样本数量也不太可能减少误差，可能合适地增加输入特征才是更有效的。而如左边的情况high variance，用更大的数据集是更有效的。
![](/机器学习/images/99.png)

1.随机梯度下降
传统的梯度下降(batch gradient descent)方式计算量太大，迭代一次效率比较低，为了解决这个问题，我们使用“随机梯度下降方式”，具体做法是：我们不再在对参数进行更新时遍历加总所有样本的误差值，而是每次迭代更新从中只随机选取一个样本进行误差计算，（本质是先将数据集打乱，再逐一进行参数学习，但还是要对所有的样本进行一次遍历），因此保证了参数是在快速地向全局最优解靠近的（但可能由于样本选取的随机性，导致梯度下降的方向并不是那么稳定地向全函数最小值处行进，相当于是牺牲稳度换取速度）。
