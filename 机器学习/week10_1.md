当数据量相当大的时候，传统迭代的方式效率太低。假设我们如果有一百万个训练样本（m=100,000,000），则每一次更新参数都需要做一百万次的加和，那么计算量就很大。另外，对于high bias的算法学习曲线（如下图的靠右图），此时增加样本数量也不太可能减少误差，可能合适地增加输入特征才是更有效的。而如左边的情况high variance，用更大的数据集是更有效的。
![](/机器学习/images/99.png)

1.随机梯度下降
传统的梯度下降(batch gradient descent)方式计算量太大，迭代一次效率比较低，为了解决这个问题，我们使用“随机梯度下降方式”（stochastic GD可），具体做法是：我们在更新参数时遍历加总所有样本的误差值，而是每次迭代更新从中只随机选取**一个样本**进行误差计算，（先将数据集打乱，再逐一进行参数学习，最终还是要对所有的样本进行一次遍历），因此保证了参数是在快速地向全局最优解靠近的（但可能由于样本选取的随机性，导致梯度下降的方向并不是那么稳定地向全函数最小值处行进，相当于是牺牲稳度换取速度）。
![](/机器学习/images/103.png)

在随机梯度下降中，每次都对1个样本进行偏差订正，如果每次对b个样本进行迭代订正，则叫小堆梯度下降（Mini-Batch Gradient Descent）
小堆梯度下降有时候可以比随机梯度下降速度更快！
小堆梯度下降其实就是将随机梯度下降中只用一个样本进行每轮迭代的做法，变成了用b个样本进行每轮迭代（此之谓mini-batch，因为每轮迭代用到的样本数量在1-m之间）